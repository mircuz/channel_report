\input{capitoli/spatial_discretization}




\section{Time discretization}
Time integration of the equations is performed by a partially-implicit method.
The use of a partially-implicit scheme is a common approach in DNS~\cite{kim_moin_moser}: the explicit part of the equations can benefit from a higher-accuracy scheme, while the stability-limiting viscous part is subjected to an implicit time advancement, thus relieving the stability constraint on the time-step size $	\Delta t$. \par
Our preferred choice, following~\cite{cpl:presentazione}\cite{kim_moin_moser} is to use an explicit third-order, low-storage Runge-Kutta method for the integration of the explicit part of the equations, and an implicit second-order Crank-Nicolson scheme is used for the implicit part. This scheme has been anyway embedded in a modular coding implementation that allows us to change the time-advancement scheme very easily without otherwise affecting the structure of the code. In fact, we have a few other time-advancement schemes built into the code for testing purposes. \par
 Here we present the time-discretized version of equations~\ref{curl:momentum:y} and~\ref{normal:velocity} for a generic wavenumber pair and a generic two-levels scheme for the explicitly-integrated part coupled with the implicit Crank-Nicolson scheme:
\begin{multline}
\frac{\lambda}{\Delta t} \hat{\eta}_{hl}^{n+1} -\frac{1}{Re} \big[ D_{2} (\hat{\eta}_{hl}^{n+1}) - k^{2} \hat{\eta}_{hl}^{n+1} \big] = \\
\frac{\lambda}{\Delta t} \hat{\eta}_{hl}^{n} + \frac{1}{Re} \big[ D_{2} (\hat{\eta}_{hl}^{n}) - k^{2} \hat{\eta}_{hl}^{n} \big] + \\
\theta \bigg( i\beta_{0}l\widehat{HU}_{hl} - i\alpha_{0}h\widehat{HW}_{hl} \bigg)^{n} + \xi \bigg( i\beta_{0}l\widehat{HU}_{hl} - i\alpha_{0}h \widehat{HW}_{hl} \bigg)^{n-1}
\end{multline}
\begin{multline}
\frac{\lambda}{\Delta t} \big( D_{2} (\hat{v}_{hl}^{n+1}) - k^{2} \hat{v}_{hl}^{n+1} \big) -\frac{1}{Re} \big[ D_{4} (\hat{v}_{hl}^{n+1}) -2 k^{2} D_{2} (\hat{v}_{hl}^{n+1}) + k^{4} \hat{v}_{hl}^{n+1} \big] =\\
\frac{\lambda}{\Delta t} \big( D_{2}(\hat{v}_{hl}^{n}) - k^{2}\hat{v}_{hl}^{n} \big) + \frac{1}{Re} \big[ D_{4} (\hat{v}_{hl}^{n}) -2 k^{2} D_{2} (\hat{v}_{hl}^{n}) + k^{4} \hat{v}_{hl}^{n} \big]+ \\
\theta \bigg( -k^{2} \widehat{HV}_{hl} -D_{1} \big( i\alpha_{0} h \widehat{HU}_{hl} + i\beta_{0} l \widehat{HW}_{hl} \big)  \bigg)^{n} + \\
\xi \bigg( -k^{2} \widehat{HV}_{hl} -D_{1} \big( i\alpha_{0}h\widehat{HU}_{hl} + i\beta_{0}l \widehat{HW}_{hl} \big) \bigg)^{n-1}
\end{multline}

The three coefficients $\lambda$, $\theta$ and $\xi$ define a particular time-advancement scheme. For the simplest case of a $2^{nd}$-order Adams-Bashfort, for example, we have $\lambda= 2$, $\theta = 3$ and $\xi = -1$.\par
The procedure to solve these discrete equations is made by two distinct steps. In the first step, the RHSs corresponding to the explicitly-integrated  part have to be assembled. In the representation~\ref{Fourier:v}, at a given time the Fourier coefficients of the variables are represented at different y positions; hence the velocity products can be computed through inverse/direct FFT in wall-parallel planes. Their spatial derivatives are then computed: spectral accuracy can be achieved for wall-parallel derivatives, whereas the finite-differences compact schemes described in~\ref{FD:scheme} are used in the wall-normal direction. These spatial derivatives are eventually combined with values of the RHS at previous time levels. The whole y~range from one wall to the other must be considered. 
\par
The second step involves, for each $\alpha,\beta$ pair, the solution of a set of two ODEs, derived from the implicitly integrated viscous terms, for which the RHS is now known. A finite-differences discretization of the wall-normal differential operators produces two real banded matrices, in particular pentadiagonal matrices when a 5-point stencil is used. The solution of the resulting two linear systems gives $\hat{\eta}_{hl}^{n+1}$ and $\hat{v}_{hl}^{n+1}$, and then the planar velocity components $\hat{u}_{hl}^{n+1}$ and $\hat{w}_{hl}^{n+1}$ can be computed by solving system~\ref{uw:eq} for each wavenumber pair. For each $\alpha,\beta$ pair, the solution of the two ODEs requires the simultaneous knowledge of their RHS in all y positions. The whole $\alpha,\beta$ space must be considered. In the $\alpha - \beta - y$ space the first step of this procedure proceeds per wall-parallel planes, while the second one proceeds per wall-normal lines~\cite{cpl:presentazione}.

\section{Domain decompositions}
The engine of Quadrio and Luchini described into~\cite{cpl:presentazione} works per \emph{y-slabs}, as shown in figure~\ref{domain_decomp},
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{grafici/decomp_dominio_cpl}
\caption{Original domain decomposition in case of 4 processors}
\label{domain_decomp}
\end{figure}
 allowing to perform convolutions and Fourier transformations locally on each processor, avoiding the cost of non-local transposition for the velocity array and the non-linear terms ones. Such implementation, denominated pipelined-linear-system (PLS), lead to a minimum in communications, in fact this approach require to send and receive only the values stored in the two upper and lower boundary cells of the slice, in order to provide and gather the data required by the fourth-order finite difference scheme, used along the y-direction.
 \par
 Using the PLS approach the bytes exchanged in a three step Range-Kutta method are:
 \begin{equation}
 D_{t} = 3 \times 8 \times (p-1) \frac{3}{2} \times \frac{nx}{p} \frac{nz}{p} \times ny \times 18
 \label{exchange:data:cpl}
 \end{equation}
 where:
 \begin{description}
  \item[3] takes into account the number of time steps;
  \item[8] for counting the bytes;
  \item[$\mathbf{(p-1)}$] is the number of nodes across which the exchange take place;
  \item[ $\mathbf{\frac{3}{2}}$ ] corresponds to the expansion in horizontal modes required by the dealiasing process;
  \item[ $\mathbf{\frac{nx}{p} \times \frac{nz}{p}}$] is the grid portion, for each plane, to exchange with the others nodes;
  \item[18] due to the 3 velocities plus the 6 products to be exchanged twice, before and after the FFT;
  \item[ny] takes into account the number of planes to be exchanged.
\end{description}
Further details about the PLS communications are available in \cite[\nopp chapter 4.2]{cpl:presentazione}. \\
 \par
Although efficient for small processors grid, the performances of this approach falls quickly whether the processors number becomes comparable with \emph{ny}.  Furthermore the communications cost limit the number of parallel process to be just a fraction of the \emph{ny} extension. \\~\\
\par
To avoid such limitations and increase the number of parallel processes we decided to move from PLS approach to something different.
We have identified two possible solutions:
\begin{description}
  \item employ \textbf{slab} decomposition along x or z axis;
  \item employ \textbf{pencil} decomposition.
\end{description}
\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{grafici/decomp_example}
\caption{Kinds of decomposition}
\label{decomposition:example}
\end{center}
\end{figure}
Both implementations require extensive use of the MPI paradigm and, possibly, a library to handle such decompositions.
\par
We opted to employ OpenMPI~\cite{openmpi} for what concern the MPI paradigm, in particular we entrusted to the well established MPI standard 3.1~\cite{MPI:standard}, using OpenMPI release 3.1.3.
The ideas behind the choice of such library rely on the fact that OpenMPI is released behind BSD license, it is designed to group different MPI implementation, avoiding fragmentation and forking problem~\cite{faq:openmpi} and, although less optimized on proprietary fabric such as Intel Omni-Path fabric~\cite{intel:omnipath}\cite{intel:intelmpivsopenmpi}, it is likely the most widespread message passing interface package.
\par
For what concern the decomposition we entrusted in a new library, released by Steven Plimpton from the Sandia National Laboratories, called \emph{fftMPI}~\cite{fftMPI}. Such library leans on the MPI implementation, providing the proper cartesian communicator needed to perform the transposition of the arrays. Next to the communicator, this brand new library provide some useful tweak like permutations or the possibility to select the desired communication mode, which, compared with the FFTW-MPI Transpose~\cite{FFTW05}\cite{FFTW:transpose} features, provide a wider personalization and reduce the work needed for the subsequent operations, such as the FFTs. 
\par
Finally is important to highlight that \emph{fftMPI} can handle both 1D and 2D decomposition, unlike FFTW-MPI.
Others similar libraries are P3DFFT~\cite{p3dfft} from professor Dmitry Pekurovsky (UCSD), PFFT from professor Michael Pippig~\cite{pfft} (Technische Universitat Chemnitz) and 2DECOMP\&FFT~\cite{2decomp} by Ning Li.





\subsection{1D decomposition}
The objective of the decomposition is to spread the computational cost across as much as tasks possible. This criteria goes against our needs to have the $y$ dimension local on chip and the requirements to exploit a 2D FFT on plane $xz$.
A reasonable tradeoff to this requirements is employ a slab decomposition. 
\par
Given an $N\times N\times N$ grid of points to be distributed over $p$ processors two approaches exist in order to carry out the 2D FFTs and arrange the data as needed.\\
The first approach is to implement a sequence of 2 one-dimensional FFTs, interleaved with data exchange among processors. The other approach, which requires less overall communication, is to make the data local for the dimensions to be transformed (PLS approach~\cite{cpl:presentazione}). \\
\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{grafici/1d_decomp}
\caption{Example of 1D decomposition: a) \emph{PLS},  b) \emph{x-slabs}}
\label{1d:decomp}
\end{center}
\end{figure}
\par
In detail we exploiting the $N_{y}$ planes independence, to loop among them. Every plane is originally decomposed in $\frac{N_{x}}{p}\times N_{z}$. The code perform the $z$-dimensional Fourier transformations and an MPI-transposition take place, leading to the new domain decomposition $N_{x} \times \frac{N_{z}}{p}$. In this arrangement the $x$-dimensional FFTs and subsequent convolutions are performed. Once completed the opposite process takes place. 
\par
Working using this kind of implementation let us to maintain local the $y$ dimension for the entire length of the process. 
\par
For a tiny cluster or problems with limited dimensions this kind of approach could be a good choice, since the decomposition limit varies with the lowest dimension, in this case, $N_{x}$ or $N_{z}$. However the speedup is limited. 
\par
Such limitations about domain decomposition and, consequently, speedup, raise their importance in modern HPC architectures, that counts many thousands cores.  
This limits can be avoided implementing a 2D decomposition.





\subsection{2D decomposition}
In 2D decomposition the tasks subdivide the grid in order to form pencils instead of slabs. The dimensions of these pencils are $N_{x}\times \frac{N_{y}}{p_{1}}\times \frac{N_{z}}{p_{2}}$ where the total amount of task is $p = p_{1}\times p_{2}$.
\par
The sequence of operations required to remap a 3D array along the three dimensions is illustrated in figure~\ref{2d:decomp}. 
\par
\begin{figure}[h]
\begin{center}
\includegraphics[width=1.08\textwidth]{grafici/2d_decomp}
\caption{2D decomposition of a 3D array}
\label{2d:decomp}
\end{center}
\end{figure}
The 2D approach raise the limit of decomposition to be $N\times N$, allowing to scale to an higher number of processes with respect to the 1D method, thus achieve an higher speedup and efficiency when the dimension of the problem become significant.


\input{capitoli/io}

\section{Third party libraries}
\input{capitoli/libraries}
